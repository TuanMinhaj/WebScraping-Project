{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to extract values in each pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title \n",
    "def get_title(soup):\n",
    "    try:\n",
    "        #outer tag object\n",
    "        title=soup.find(\"span\",attrs={\"id\":'productTitle'}).get_text().strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title=\"\"\n",
    "    return title\n",
    "\n",
    "#Function to extract Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price=soup.find('span',attrs={\"class\":\"a-offscreen\"}).get_text().strip()[1:]\n",
    "        # price=float(price)\n",
    "    except AttributeError:\n",
    "        price=''\n",
    "    return price\n",
    "\n",
    "\n",
    "#Function to extract ratings\n",
    "def get_star(soup):\n",
    "    try:\n",
    "        star=soup.find(\"i\",attrs={\"class\":\"a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big\"}).get_text().strip().split()[0]\n",
    "        # star=float(star)\n",
    "    except AttributeError:\n",
    "        star=''\n",
    "    return star\n",
    "\n",
    "#Function to extract review counts\n",
    "def get_reviews(soup):\n",
    "    try:\n",
    "        review=soup.find(\"span\",attrs={\"id\":\"acrCustomerReviewText\"}).get_text().strip().split()[0]\n",
    "        # review=float(review)\n",
    "    except AttributeError:\n",
    "        review=''\n",
    "    return review\n",
    "\n",
    "#Function to extract availability\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        availability=soup.find(\"div\",attrs={\"id\":\"availability\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        availability=''\n",
    "    return availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__';\n",
    "\n",
    "    #url of the desired page\n",
    "    URL= \"https://www.amazon.com/s?k=shirt&crid=3LPBRJJZ7BSJ1&qid=1686681187&sprefix=shir%2Caps%2C555&ref=sr_pg_1\"\n",
    "\n",
    "    #headers for the requests\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    #Http Request\n",
    "    page = requests.get(URL, headers=headers)           #bring data to the local environment\n",
    "\n",
    "    #Soup objects containing all data as html \n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")         #retrieve data as html contents\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")     #showcase html data as better indentation\n",
    "\n",
    "    #fetch link as list of tag objects\n",
    "    links=soup2.find_all('a',attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})    #finding all anchor tags to extract url links in the web page. this gives as a block\n",
    "\n",
    "    links_list=[]\n",
    "    for link in links:\n",
    "        links_list.append(link.get('href'))\n",
    "\n",
    "    \n",
    "    d={\"title\":[],\"price\":[],\"rating\":[],\"reviews\":[],\"availability\":[]}\n",
    "\n",
    "    for link in links_list:\n",
    "        new_webpage=requests.get(\"https://www.amazon.com\"+link,headers=headers)\n",
    "        new_soup=BeautifulSoup(new_webpage.content,'html.parser')\n",
    "        new_soup2=BeautifulSoup(new_soup.prettify(),'html.parser')\n",
    "        \n",
    "        d['title'].append(get_title(new_soup2))\n",
    "        d['price'].append(get_price(new_soup2))\n",
    "        d['rating'].append(get_star(new_soup2))\n",
    "        d['reviews'].append(get_reviews(new_soup2))\n",
    "        d['availability'].append(get_availability(new_soup2))\n",
    "\n",
    "\n",
    "amazon_df=pd.DataFrame.from_dict(d)\n",
    "amazon_df\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, price, rating, reviews, availability]\n",
       "Index: []"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m soup2 \u001b[39m=\u001b[39m BeautifulSoup(soup1\u001b[39m.\u001b[39mprettify(), \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)     \u001b[39m#showcase html data as better indentation\u001b[39;00m\n\u001b[0;32m      9\u001b[0m links\u001b[39m=\u001b[39msoup2\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m,attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39ma-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\u001b[39m\u001b[39m'\u001b[39m})    \u001b[39m#finding all anchor tags to extract url links in the web page. this gives as a block\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m item_link\u001b[39m=\u001b[39mlinks[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m)       \u001b[39m#this will extract url in a particular item \u001b[39;00m\n\u001b[0;32m     12\u001b[0m product_link\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://amazon.com\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mitem_link    \u001b[39m#this will give direct accessible browser url \u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# print(links)  \u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#connecting to the website and scrape all the data from the website and take into local\n",
    "\n",
    "URL= \"https://www.amazon.com/s?k=shirt&crid=3LPBRJJZ7BSJ1&sprefix=shir%2Caps%2C555&ref=nb_sb_noss_2\"    #url of the desired page\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "page = requests.get(URL, headers=headers)           #bring data to the local environment\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")         #retrieve data as html contents\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")     #showcase html data as better indentation\n",
    "links=soup2.find_all('a',attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})    #finding all anchor tags to extract url links in the web page. this gives as a block\n",
    "\n",
    "item_link=links[0].get('href')       #this will extract url in a particular item \n",
    "product_link=\"https://amazon.com\"+item_link    #this will give direct accessible browser url \n",
    "\n",
    "# print(links)  \n",
    "print(product_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Stock'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing for obtain product url\n",
    "\n",
    "product_page = requests.get(product_link, headers=headers)\n",
    "soup3 = BeautifulSoup(product_page.content, \"html.parser\")         #retrieve data as html contents specified page\n",
    "soup4 = BeautifulSoup(soup3.prettify(), \"html.parser\")            \n",
    "\n",
    "title=soup4.find(id='productTitle').get_text().strip()              #get id to get title with removing blanks\n",
    "\n",
    "price=soup4.find('span',attrs={\"class\":\"a-offscreen\"}).get_text().strip()[1:]  #get price value as str\n",
    "price=float(price)\n",
    "\n",
    "star=soup4.find(\"i\",attrs={\"class\":\"a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big\"}).get_text().strip().split()[0]\n",
    "star=float(star)\n",
    "\n",
    "review=soup4.find(\"span\",attrs={\"id\":\"acrCustomerReviewText\"}).get_text().strip().split()[0]\n",
    "# review\n",
    "\n",
    "availability=soup4.find(\"div\",attrs={\"id\":\"availability\"}).get_text().strip()\n",
    "availability\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "today=datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating csv files and storing data in the csv file\n",
    "import csv\n",
    "\n",
    "headers=['Title','Price','Date']\n",
    "data=[title,price,today]\n",
    "# print(data)\n",
    "\n",
    "with open('Amazon_Web_Scraper_Dataset.csv','w',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerow(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Price        Date\n",
       "0  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating dataframes to store data in the csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r'C:\\Users\\tuanminhaj\\Data-Analyst-Projects\\WebScraping-Project\\Amazon_Web_Scraper_Dataset.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending data to the csv \n",
    "\n",
    "with open('Amazon_Web_Scraper_Dataset.csv','a+',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3309443741.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    with smtplib.SMTP_SSL('smtp.gmail.com',465,context=context) as smtp;\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#this is sending emails to customer who are interested when price is low \n",
    "\n",
    "current_dir=Path(__file__).resolve().parent if \"__file__\"in locals() else Path.cwd()\n",
    "envars=current_dir/\".env\"\n",
    "load_dotenv(envars)\n",
    "\n",
    "email_sender=os.getenv(\"sender_email\")\n",
    "email_password=os.getenv(\"password\")\n",
    "email_receiver='tuanminhaj1996@gmail.com'\n",
    "\n",
    "def send_mail():\n",
    "    server=smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    # server.login('tuanminhajseedin@gmail.com','xxxxxxxxxx')\n",
    "\n",
    "    subject='The shirt you want is below $15! Now is your chance to buy!'\n",
    "    body=\"Tuan, This is the moment we have been waiting for, now is your chance to pick this great chance. Don't miss this chance\"\n",
    "\n",
    "    # msg=f\"Subject:{subject}\\n\\n{body}\"\n",
    "\n",
    "    em=EmailMessage()\n",
    "    em['From']=email_sender\n",
    "    em[\"To\"]=email_receiver\n",
    "    em[\"Subject\"]= subject\n",
    "    em.set_content(body)\n",
    "\n",
    "    context=ssl.create_default_context()\n",
    "\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com',465,context=context) as smtp;\n",
    "        smtp.login(email_sender,email_password)\n",
    "        smtp.sendmail(email_sender,email_receiver,em.as_string())\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automating appending data to insert into dataframe itself within given time boundary\n",
    "\n",
    "def check_price():\n",
    "  \n",
    "    url='https://www.amazon.com/Data-Analyst-T-Shirt-Male-Boss/dp/B09T5JC2XM/ref=sr_1_1?crid=3QE0W4BK4GHFC&keywords=data%2Banalyst%2Bshirt&qid=1683299953&sprefix=data%2Banalyst%2Bs%2Caps%2C689&sr=8-1&th=1&psc=1'\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    page=requests.get(url,headers=headers)\n",
    "    soup1=bs(page.content,\"html.parser\")   \n",
    "    soup2=bs(soup1.prettify(),\"html.parser\")   \n",
    "\n",
    "    title=soup2.find(id='productTitle').get_text()\n",
    "    price=soup2.find(id=\"corePriceDisplay_desktop_feature_div\").get_text()\n",
    "\n",
    "    price=price.strip()[1:6]  \n",
    "    title=title.strip()\n",
    "\n",
    "    today=datetime.date.today()\n",
    "    \n",
    "    headers=['Title','Price','Date']\n",
    "    data=[title,price,today]\n",
    "\n",
    "    with open('Amazon_Web_Scraper_Dataset.csv','a+',newline='',encoding='UTF8') as f:\n",
    "        writer=csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "\n",
    "    if float(price)<14:\n",
    "        send_mail()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     check_price()\n\u001b[1;32m----> 3\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    check_price()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst T-Shirt for Men,Him, Male, Boss, ...</td>\n",
       "      <td>25.75</td>\n",
       "      <td>2023-05-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Price        Date\n",
       "0  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "1  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "2  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "3  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "4  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "5  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "6  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "7  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06\n",
       "8  Data Analyst T-Shirt for Men,Him, Male, Boss, ...  25.75  2023-05-06"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\tuanminhaj\\Data-Analyst-Projects\\WebScraping-Project\\Amazon_Web_Scraper_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
