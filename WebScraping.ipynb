{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to extract values in each pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title \n",
    "def get_title(soup):\n",
    "    try:\n",
    "        #outer tag object\n",
    "        title=soup.find(\"span\",attrs={\"id\":'productTitle'}).get_text().strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title=\"\"\n",
    "    return title\n",
    "\n",
    "#Function to extract Price\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price=soup.find('span',attrs={\"class\":\"a-offscreen\"}).get_text().strip()[1:]\n",
    "        # price=float(price)\n",
    "    except AttributeError:\n",
    "        price=''\n",
    "    return price\n",
    "\n",
    "\n",
    "#Function to extract ratings\n",
    "def get_star(soup):\n",
    "    try:\n",
    "        star=soup.find(\"i\",attrs={\"class\":\"a-icon a-icon-star a-star-4-5 cm-cr-review-stars-spacing-big\"}).get_text().strip().split()[0]\n",
    "        # star=float(star)\n",
    "    except AttributeError:\n",
    "        star=''\n",
    "    return star\n",
    "\n",
    "#Function to extract review counts\n",
    "def get_reviews(soup):\n",
    "    try:\n",
    "        review=soup.find(\"span\",attrs={\"id\":\"acrCustomerReviewText\"}).get_text().strip().split()[0]\n",
    "        # review=float(review)\n",
    "    except AttributeError:\n",
    "        review=''\n",
    "    return review\n",
    "\n",
    "#Function to extract availability\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        availability=soup.find(\"div\",attrs={\"id\":\"availability\"}).get_text().strip()\n",
    "    except AttributeError:\n",
    "        availability=''\n",
    "    return availability\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Csv file with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [503]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#url of the desired page\n",
    "URL= \"https://www.amazon.com/s?k=tshirt&crid=12L97UNZ3NXOS&sprefix=tsh%2Caps%2C1716&ref=nb_sb_noss_2\"\n",
    "\n",
    "#headers for the requests\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "#Http Request\n",
    "page = requests.get(URL, headers=headers)           #bring data to the local environment\n",
    "\n",
    "#Soup objects containing all data as html \n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")         #retrieve data as html contents\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")     #showcase html data as better indentation\n",
    "\n",
    "\n",
    "#fetch link as list of tag objects\n",
    "links=soup2.find_all('a',attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})    #finding all anchor tags to extract url links in the web page. this gives as a block\n",
    "\n",
    "links_list=[]\n",
    "for link in links:\n",
    "    links_list.append(link.get('href'))\n",
    "\n",
    "#making empty dictionary to convert as dataframe\n",
    "d={\"title\":[],\"price\":[],\"rating\":[],\"reviews\":[],\"availability\":[]}\n",
    "\n",
    "#modify each link as web brower links to achieve each item web pages\n",
    "for link in links_list:\n",
    "    new_webpage=requests.get(\"https://www.amazon.com\"+link,headers=headers)\n",
    "    new_soup=BeautifulSoup(new_webpage.content,'html.parser')\n",
    "    new_soup2=BeautifulSoup(new_soup.prettify(),'html.parser')\n",
    "    get_title(soup2)\n",
    "\n",
    "    \n",
    "    #appending data to the empty dictionary\n",
    "    d['title'].append(get_title(new_soup2))\n",
    "    d['price'].append(get_price(new_soup2))\n",
    "    d['rating'].append(get_star(new_soup2))\n",
    "    d['reviews'].append(get_reviews(new_soup2))\n",
    "    d['availability'].append(get_availability(new_soup2))\n",
    "\n",
    "        \n",
    "\n",
    "# print(page)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Frames with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converet dictionary to datafram\n",
    "amazon_df=pd.DataFrame.from_dict(d)\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "#convert dataframe to csv\n",
    "amazon_df.to_csv('Amazon.csv')\n",
    "\n",
    "#reading csv in pandas\n",
    "df=pd.read_csv(\"./Amazon.csv\").head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>availability</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, price, rating, reviews, availability, Date]\n",
       "Index: []"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "#Handling Null values \n",
    "df['availability'].fillna(\"out of stocks\",inplace=True)\n",
    "\n",
    "#drop unnessary columns\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "# df\n",
    "today=datetime.date.today()\n",
    "df['Date']=today\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
