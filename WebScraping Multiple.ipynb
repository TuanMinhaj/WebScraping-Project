{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests       \n",
    "import smtplib        #sending emails\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd    #making Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sending mail for users if price is reduced \n",
    "\n",
    "def send_mail():\n",
    "    server=smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
    "    server.ehlo()\n",
    "    server.login('tuanminhajseedin@gmail.com','xxxxxxxxxx')\n",
    "\n",
    "    subject='The shirt you want is below $15! Now is your chance to buy!'\n",
    "    body=\"Tuan, This is the moment we have been waiting for, now is your chance to pick this great chance. Don't miss this chance\"\n",
    "\n",
    "    msg=f\"Subject:{subject}\\n\\n{body}\"\n",
    "\n",
    "    server.sendmail(\n",
    "        'tuanminhajseedin@gmail.com',msg\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 01 for load data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate a csv file to load the scraped data\n",
    "headers=['Title','Star','Price','Availability']\n",
    "with open('books_Web_Scraper_Dataset.csv','w',newline='',encoding='UTF8') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to the website and scrape all the data from the website and take into local\n",
    "books=[]\n",
    "def book_details():\n",
    "    for i in range(1,51):\n",
    "        url=f'https://books.toscrape.com/catalogue/page-{i}.html'\n",
    "        # print(url)\n",
    "\n",
    "        page=requests.get(url)\n",
    "        soup1=bs(page.content,\"html.parser\")   #pulling all the html from the website\n",
    "        \n",
    "        #find searching under tag name of ol(order_list) and find by article with required class\n",
    "        order_list=soup1.find('ol')\n",
    "        articles=order_list.find_all('article',class_='product_pod')\n",
    "\n",
    "        \n",
    "        \n",
    "        #retrieve all the titles inside the articles and taking the all titles in the first page\n",
    "        for article in articles:        #looping through each articles\n",
    "            image=article.find('img')       #find all img tags\n",
    "            title=image.attrs['alt']        #filter out the title with it attributes\n",
    "            star=article.find('p')          #star ratings are under p tags we can straight scrape it\n",
    "            star=star['class'][1]           #to retrieve only rating value we need to get 1 index values\n",
    "            price=article.find('p',class_='price_color').text     #finding the price values, those are under p tags with class of price_color and filter it's text value\n",
    "            price=float(price[1:])             #text values are return with strings. we need to change it's type as float to store in csv\n",
    "            availability=article.find('p',class_='instock availability').text\n",
    "            availability=availability.strip()    #remove unnessary spaces \n",
    "\n",
    "            books.append([title,star,price,availability])\n",
    "        \n",
    "            data=[title,star,price,availability]\n",
    "\n",
    "            with open('books_Web_Scraper_Dataset.csv','a+',newline='',encoding='UTF8') as f:    #appending all the data into the csv file\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerow(data)\n",
    "\n",
    "\n",
    "            if price<50:                      #sending a mail for user if price is reduced\n",
    "                send_mail()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 for load data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making Dataframes\n",
    "\n",
    "df=pd.DataFrame(books,columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe into csv file format\n",
    "\n",
    "df.to_csv('books.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate the process in every single day and update the csv file\n",
    "while True:                           \n",
    "    book_details()\n",
    "    time.sleep(3600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
